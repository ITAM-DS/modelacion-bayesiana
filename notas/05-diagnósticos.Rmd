---
output:
  pdf_document: default
  html_document: default
---

# Diagnósticos 


```{r, include=FALSE, message=FALSE}
library(tidymodels)
library(tidyverse)
library(cmdstanr)
library(rstanarm)
library(bayesplot)
library(loo)

library(patchwork)
library(scales)
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning=FALSE, 
                      fig.align = 'center', fig.width = 5, fig.height=3, cache = TRUE)
comma <- function(x) format(x, digits = 2, big.mark = ",")
theme_set(theme_linedraw())
color.blues <- c(NA,"#BDD7E7", "#6BAED6", "#3182BD", "#08519C", "#074789", "#063e77", "#053464")
color.itam  <- c("#00362b", "#00503f", "#006953", "#008367", "#009c7b", "#00b68f")


sin_lineas <- theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
sin_leyenda <- theme(legend.position = "none")
sin_ejes <- theme(axis.ticks = element_blank(), 
                  axis.text = element_blank())
```

## Resiudales {-}

```{r}
kidiq <- read_csv("../datos/kidiq.csv")
kidiq %>% head()
```

```{r}
kidiq <- kidiq %>% mutate(mom_iq_c = mom_iq - mean(mom_iq))

fit_kid <- stan_glm(kid_score ~ mom_iq_c, data=kidiq, refresh = 0)
print(fit_kid)
```

```{r}

kidiq %>% 
    mutate( residuals = kid_score - predict(fit_kid)) %>% 
    ggplot(aes(x = mom_iq, y = residuals)) + 
        geom_point() + sin_lineas + 
        geom_hline(yintercept = 0, lty = 2, size = 1) + 
        # geom_hline(yintercept = c(18.3, -18.3), lty = 2) + 
        geom_ribbon(aes(ymin = -18.3, ymax = 18.3), alpha = .3) + 
        geom_ribbon(aes(ymin = -2 * 18.3, ymax = 2 * 18.3), alpha = .3) + 
        ggtitle("Residuales contra predictor")

```

```{r}

g1 <- kidiq %>% 
    mutate( residuals  = kid_score - predict(fit_kid), 
            prediccion = predict(fit_kid)) %>% 
    ggplot(aes(x = prediccion, y = residuals)) + 
        geom_point() + sin_lineas + 
        geom_hline(yintercept = 0, lty = 2, size = 1) + 
        # geom_hline(yintercept = c(18.3, -18.3), lty = 2) + 
        geom_ribbon(aes(ymin = -18.3, ymax = 18.3), alpha = .3) + 
        geom_ribbon(aes(ymin = -2 * 18.3, ymax = 2 * 18.3), alpha = .3) + 
        ggtitle("Residuales contra prediccion")
    
g2 <- kidiq %>% 
    mutate( residuals  = kid_score - predict(fit_kid), 
            prediccion = predict(fit_kid)) %>% 
    ggplot(aes(x = kid_score, y = residuals)) + 
        geom_point() + sin_lineas + 
        geom_hline(yintercept = 0, lty = 2, size = 1) + 
        # geom_hline(yintercept = c(18.3, -18.3), lty = 2) + 
        geom_ribbon(aes(ymin = -18.3, ymax = 18.3), alpha = .3) + 
        geom_ribbon(aes(ymin = -2 * 18.3, ymax = 2 * 18.3), alpha = .3) + 
        ggtitle("Residuales contra observación")

g1 + g2
```

```{r}

a <- 0.6    
b <- 86.8
sigma <- 18.3

kidiq_sim <- tibble(mom_iq_c    = kidiq$mom_iq_c, 
                     kid_score = a * kidiq$mom_iq_c + b + 18.3 * rnorm(nrow(kidiq)))

fit_sim <- stan_glm(kid_score ~ mom_iq_c, data=kidiq_sim, refresh = 0)
print(fit_sim)

```

```{r}
g1 <- kidiq_sim %>% 
    mutate( residuals  = kid_score - predict(fit_sim), 
            prediccion = predict(fit_sim)) %>% 
    ggplot(aes(x = prediccion, y = residuals)) + 
        geom_point() + sin_lineas + 
        geom_hline(yintercept = 0, lty = 2, size = 1) + 
        # geom_hline(yintercept = c(18.3, -18.3), lty = 2) + 
        geom_ribbon(aes(ymin = -18.3, ymax = 18.3), alpha = .3) + 
        geom_ribbon(aes(ymin = -2 * 18.3, ymax = 2 * 18.3), alpha = .3) + 
        ggtitle("Residuales contra prediccion")
    
g2 <- kidiq_sim %>% 
    mutate( residuals  = kid_score - predict(fit_sim), 
            prediccion = predict(fit_sim)) %>% 
    ggplot(aes(x = kid_score, y = residuals)) + 
        geom_point() + sin_lineas + 
        geom_hline(yintercept = 0, lty = 2, size = 1) + 
        # geom_hline(yintercept = c(18.3, -18.3), lty = 2) + 
        geom_ribbon(aes(ymin = -18.3, ymax = 18.3), alpha = .3) + 
        geom_ribbon(aes(ymin = -2 * 18.3, ymax = 2 * 18.3), alpha = .3) + 
        ggtitle("Residuales contra observación")

g1 + g2
```

## Evaluación de la predictiva posterior {-}

```{r}

newcomb <- read_table("../datos/newcomb")
newcomb %>% head()

```

```{r}

newcomb %>% 
    ggplot(aes(x = y)) + 
        geom_histogram() + sin_lineas

```

```{r}

fit_newc <- stan_glm(y ~ 1, data=newcomb, refresh=0)
fit_newc

```

```{r}

y_rep <- posterior_predict(fit_newc)

```

```{r}

ppc_hist(newcomb$y, y_rep[1:19, ], binwidth = 8) + sin_lineas

```

```{r}

ppc_dens_overlay(newcomb$y, y_rep[1:100, ]) + sin_lineas

```

```{r}

ppc_stat(newcomb$y, y_rep, stat = "min", binwidth = 2) + sin_lineas

```

```{r}

unemp <- read_table("../datos/unemployment")
unemp %>% head()

```

```{r}

unemp %>% 
    ggplot(aes(year, y)) + 
        geom_line() + sin_lineas + 
        ylab("Unemployment rate (%)")

```
```{r}

fit_lag <- stan_glm(y ~ y_lag, data=unemp %>% mutate(y_lag = lag(y)), refresh=0)
fit_lag

```

```{r}
y_rep <- posterior_predict(fit_lag)
y_rep <- cbind(unemp$y[1], y_rep)
n_sims <- nrow(y_rep)
```

```{r}

as_tibble(y_rep) %>% 
    mutate(sim = 1:n_sims) %>% 
    sample_n(15) %>% 
    pivot_longer(cols = V1:70) %>% 
    mutate(year = rep(unemp$year, 15)) %>% 
    ggplot(aes(x = year, y = value)) + 
        geom_line() + 
        facet_wrap(~sim, ncol = 5)

```

```{r}

test <- function (y){
  n <- length(y)
  y_lag <- c(NA, y[1:(n-1)])
  y_lag_2 <- c(NA, NA, y[1:(n-2)])
  return(sum(sign(y-y_lag) != sign(y_lag-y_lag_2), na.rm=TRUE))
}
test_y <- test(unemp$y)
test_rep <- apply(y_rep, 1, test)
print(mean(test_rep > test_y))

```

```{r}
print(quantile(test_rep, c(.1,.5,.9)))
```

```{r}
ppc_stat(y=unemp$y, yrep=y_rep, stat=test, binwidth = 1) + sin_lineas
```

## Desviación estándar de los residuales $\sigma$ y varianza explicada $R^2$ {-}

$$ \hat R = 1 - \frac{\hat \sigma2}{\sigma_y^2} \,.$$
```{r}

data <- tibble(x = 1:5 - 3, 
               y = c(1.7, 2.6, 2.5, 4.4, 3.8) - 3)

summary(ols <- lm(y ~ x, data))

```

```{r}


fit_bayes <- stan_glm(y ~ x, data = data,
  prior_intercept = normal(0, 0.2, autoscale = FALSE),
  prior = normal(1, 0.2, autoscale = FALSE),
  prior_aux = NULL,
  seed = 108727, refresh = 0
)

c(OLS   = var(predict(ols))/var(data$y), 
  Bayes = var(predict(fit_bayes))/var(data$y))

```

```{r}

bayesR2 <- bayes_R2(fit_bayes)

mcmc_hist(data.frame(bayesR2), binwidth=0.02)  +
    xlab('Bayesian R2') +
    geom_vline(xintercept=median(bayesR2)) + sin_lineas

```

```{r}

bayesR2 <- bayes_R2(fit_kid)

g1_kid <- mcmc_hist(data.frame(bayesR2), binwidth=0.01)  +
    xlab('Bayesian R2') +
    geom_vline(xintercept=median(bayesR2)) + sin_lineas + 
    xlim(0, .35) + ggtitle("Modelo regresión")

g1_kid

```

```{r}

n <- nrow(kidiq)
kidiqr <- kidiq
kidiqr$noise <- array(rnorm(5*n), c(n,5))

```

```{r}
fit_kid_noise <- stan_glm(kid_score ~ mom_hs + mom_iq_c + noise, data=kidiqr,
                   seed=108727, refresh=0)
print(fit_kid_noise)
```

```{r}

c(median(bayesR2), median(bayesR2n<-bayes_R2(fit_kid_noise)))

```

```{r}

g2_kid <- mcmc_hist(data.frame(bayesR2n), binwidth=0.01)  +
    xlab('Bayesian R2') +
    geom_vline(xintercept=median(bayesR2n)) + sin_lineas + xlim(0, .35) + 
    ggtitle("Modelo con malos predictores")

g1_kid / g2_kid


```

## Evaluación de modelos {-}

En esta sección, vamos a conocer algunas métricas para lo siguiente:

1. Evaluar la capacidad predictiva de los modelos Bayesianos.
2. Comparar los modelos Bayesianos.

Tomando en cuenta que, la predicción es una distribución y no sólo un valor puntual.

### Métricas:

* Log-densidad predictiva posterior: 

$$log \ \pi_{post}(y)= log \int \pi(y|\theta)\,\,\pi_{post}(\theta)\,\,\text{d}\theta,$$

  donde: $\pi_{post}(\theta)=\pi(\theta|\underline{y}_{n})$

* Devianza:

$$ D(\pi)=-2\,\,log \ \pi_{post}(y).$$

Es importante recordar que, en la métrica de Log-densidad predictiva posterior, buscamos valores altos por lo que, entre mayor sea su valor, mejor la capacidad predictiva del modelo, mientras que en la devianza buscamos valores bajos.

### ¿Cómo usar estas métricas?

1. Utilizando las muestras de entrenamiento para evaluar la distribución predictiva posterior. La desventaja de esta opción es que estaríamos evaluando el desempeño del modelo empleando los datos que se usaron para su ajuste por lo que pudieramos caer en un problema de sobreajuste del modelo.

2. Utilizando una distribución predictiva ajustada.

3. Utilizando validación cruzada. En esta opción también se usan los datos de entrenamiento para evaluar el desempeño del modelo, sin embargo, se particionan los datos de tal manera que con un subconjunto de ellos se ajusta el modelo y con el subconjunto complementario se evalua. La desventaja de esta opción es que es un proceso computacionalmente costoso.

## Criterios de información y desempeño de modelos {-}

### Criterio de información de Akaike (AIC)

$$\hat{ELPD}_{AIC}= log \ \pi(y|\hat{\theta}_{MLE})-k.$$

En términos de la devianza:

$$AIC= -2 \,\, log \ \pi(y|\hat{\theta}_{MLE})+2k,$$

donde:
k= número de parámetros estimados.

Este criterio cae en el caso de una distribución predictiva ajustada por el número de parámetros que se están estimando (k), de tal forma que si estamos estimando muchos parámetros, esto afecta el criterio de devianza.

**¿Por qué estamos usando el estimador por máxima verosimilitud?**

Resultado asintótico: la posterior es dominada por la verosimilitud si el número de datos es muy grande.

Esto es porque recordemos que:

$$\pi(\theta|\underline{y}_{n}) \propto \pi(\theta) [\pi(y_{1}|\theta)\pi(y_{2}|\theta)...\pi(y_{n}|\theta)].$$

Es por esto que, entre más datos tengamos, la distribución posterior se va a parecer más al componente de verosimilitud. 

Entonces, la marginal la estamos resumiendo con sólo el punto que maximiza a la distribución posterior:

$$\hat\pi_{post}(y)=\pi(y|\hat{\theta}_{MLE}).$$

### Criterio de información con Devianza (DIC)

$$\hat{ELPD}_{DIC}=log \ \pi(y|\hat{\theta}_{Bayes})-P_{DIC},$$

Donde:
$$P_{DIC}=2\,[log\,\pi(y|\hat\theta_{Bayes})-\mathbb{E}_{post}(log\,\pi(y|\theta))].$$
$$\hat{\theta}_{Bayes}= \mathbb{E}[\theta|\underline{y}_n]=\mathbb{E}_{post}(\theta).$$

El problema de este criterio es que si la distribución no es simétrica, $P_{DIC}$ podría ser negativa.

En su lugar, podemos tomar:
$$P_{DIC}=2\,\mathbb{V}_{post}(log\,\pi(y|\theta)).$$

En términos de la devianza:

$$DIC=-2\,log \ \pi(y|\hat{\theta}_{Bayes})+2P_{DIC}.$$

Este criterio se encuentra en un punto intermedio, dado que utiliza un estimador puntual ($\hat{\theta}_{Bayes}$) de la distribución posterior e incorpora la variabilidad de los datos ($\mathbb{V}_{post}(log\,\pi(y|\theta))$).

### Criterio de información con Watanabe-Akaike (WAIC)

$$\hat{ELPD}_{WAIC}=LPPD-P_{WAIC},$$
Donde:
$$P_{WAIC}=\sum_{i=1}^{n}\,\mathbb{V}_{post}(log\,\pi(y_{i}|\theta)).$$

$$LPPD=\sum_{i=1}^{n}\,log\,\pi_{post}(y_{i}).$$

En términos de la devianza:

$$WAIC=-2\,LPPD+2P_{WAIC}.$$

Estos criterios, como se había mencionado anteriormente, se basan en la distribución predictiva pero se ajustan por la complejidad del modelo.

## Validación cruzada

Queremos saber cómo se comportan nuestros modelos con datos nuevos pero cuando éstos no los tenemos por falta de recursos, tiempo, etc., podemos ajustarlos con un subconjunto de los datos de entrenamiento y validar con el subconjunto complementario.

En validación cruzada de k bloques se dividen los datos de entrenamiento en k subconjuntos de datos (también conocido como iteraciones). Se entrena un modelo en todos menos uno (k-1) de los subconjuntos y evalua el modelo en el subconjunto que no se ha utilizado para el entrenamiento. Este proceso se repite k veces, con un subconjunto diferente reservado para la evaluación (y excluido del entrenamiento) cada vez.

En términos bayesianos:

$$ log\, \pi_{ent}(\underline{y}_{val})=log \int \pi(\underline{y}_{val}|\theta)\,\,\pi_{ent}(\theta)\,\,\text{d}\theta. $$

Es decir, la log-densidad calculada con los datos de entrenamiento pero utilizando los datos de validación para medir el desempeño del modelo.

Si $k=n$ (Leave One Out -Loo):

$$LPPD_{Loo}=\sum_{i=1}^{n}\log \pi_{-i}(y_{i})=\sum_{i=1}^{n}\log \left(\frac{1}{S}\sum_{s=1}^{S}\pi(y_{i}|\theta_{-i}^s)\right),$$

donde: 

S=número de simulaciones de la posterior y
$\theta_{-i}^s$= muestras de la distribución posterior que **no** utiliza la observación $y_{i}.$

Si el número de datos es pequeño entonces, necesitamos una corrección que nos indique qué tanto habría mejorado la predicción si hubieramos usado todas las observaciones:

$$b= LPPD-\overline{LPPD}_{-i},$$

donde:

$$\overline{LPPD}_{-i}=\frac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{n}\log \pi_{-i}(y_{j}).$$

Entonces, $LPPD_{Loo}$ corregido, sería:

$$LPPD_{CLoo}=LPPD_{Loo}+b.$$

Con lo anterior podemos estimar el número de parámetros efectivos:

$$P_{Loo}=LPPD-LPPD_{Loo}.$$

Haciendo la corrección:

$$P_{CLoo}=LPPD-LPPD_{CLoo}=\overline{LPPD}_{-i}-LPPD_{Loo}.$$

Necesitamos calcular:

$$\frac{1}{S}\sum_{s=1}^{S}\pi(y_{i}|\theta_{-i}^s),$$

donde:

$$\theta_{-i}^s \sim \pi_{-i}(\theta).$$

Sabemos que:

$$\pi(\theta|\underline{y}_{n}) \propto \pi(\theta) \pi(y_{1}|\theta)...\pi(y_{n}|\theta) = \pi(\theta) [\pi(y_{1}|\theta)...\pi(y_{n-1}|\theta)]\pi(y_{n}|\theta) \propto \pi(\theta|\underline{y}_{n-1})\pi(y_{n}|\theta).$$

**Nota:** Lo anterior se pudo haber realizado con cualquier observación i-ésima.

Con estimación Monte-Carlo teníamos que:

$$\frac{1}{S} \sum h(\theta^s),$$

donde: $\theta^s \sim \pi(\theta)$

Lo que se está diciendo es que, a cada simulación se le asigna un peso $(\theta^s,w^s)=(\theta^s,\frac{1}{S})$

Si se genera una muestra de la posterior junto con su peso y despúes se cambia el peso a $\frac{1/S}{\pi(y_{i}|\theta)}$, lo que estamos haciendo es obtener muestras de la posterior **sin** la observación `i`, es decir, $(\theta_{-i}^s,w_{-i}^s)$ y entonces:

$$\theta_{-i}^s \sim \pi_{-i}(\theta).$$

Este resultado nos indica que podemos **reutilizar las muestras de la distribución posterior calculadas inicialmente**, cambiarles el peso a como se indicó en el párrafo anterior para así pretender tener muestras de la distribución posterior sin utilizar la observación `i`.

Con esto, la estimación Monte-Carlo sería:

$$\frac{1}{S\pi(y_{i}|\theta)} \sum h(\theta_{-i}^s)=\mathbb{E}_{-i}(h(\theta)).$$

**Observaciones:**

1. Este método en papel es computacionalmente costoso.
2. En la práctica es baráto porque hacemos un cambio sencillo a las muestras de la distribución posterior que ya se tenían.
3. Tiene diagósticos que nos indican cuando la estimación de devianza es estable.

Veamos un ejemplo con unos datos ficticios.

```{r}
SEED <- 2141
set.seed(SEED) 

x <- 1:20
n <- length(x)
a <- 0.2
b <- 0.3
sigma <- 1
y <- a + b*x + sigma*rnorm(n)
fake <- data.frame(x, y)

head(fake)

```
Ajustamos modelo lineal

```{r}

fit_all <- stan_glm(y ~ x, data = fake, seed=SEED, chains=10, refresh=0)
print(fit_all)

```

Ajustamos modelo sin la observación 18

```{r}

fit_minus_18 <- stan_glm(y ~ x, data = fake[-18,], seed=SEED, refresh=0)
print(fit_minus_18)

```

No utilizar una observación en el ajuste del modelo nos está dando un modelo relativamente sobreajustado dado que la estimación de sigma pasa de 1 a 0.9 al quitar la observación 18.

Extraemos muestras de la posterior

```{r}
# Modelo completo
sims <- as.matrix(fit_all)

# Modelo sin observación
sims_minus_18 <- as.matrix(fit_minus_18)

```

Calculamos la distribución predictiva posterior para $x = 18$

```{r}

predpost <- tibble(y = seq(0,9,length.out=100)) %>% 
  mutate(x = map(y, ~mean(dnorm(., sims[,1] + sims[,2] * 18, sims[,3])*6+18))) %>% 
  unnest(x)

```

Calculamos la predictiva posterior (LOO) para $x = 18$

```{r}

predpost.loo <- tibble(y = seq(0,9,length.out=100)) %>% 
  mutate(x = map(y, ~mean(dnorm(., sims_minus_18[,1] + sims_minus_18[,2] * 18, 
                                sims_minus_18[,3])*6+18))) %>% 
  unnest(x)

```

Graficamos 

```{r}

p.datos <- ggplot(fake, aes(x = x, y = y)) +
  geom_point(color = "white", size = 3) +
  geom_point(color = "black", size = 2) + sin_lineas

p.modelo <- p.datos +
  geom_abline(
    intercept = mean(sims[, 1]),
    slope = mean(sims[, 2]),
    size = 1,
    color = "black"
  )

p.predpost <- p.modelo + 
  geom_path(data=predpost,aes(x=x,y=y), color="black") +
  geom_vline(xintercept=18, linetype=3, color="grey")

```

Agregamos la predicción con modelo incompleto (LOO)

```{r}

p.predloo <- p.predpost +
  geom_point(data=fake[18,], color = "grey50", size = 5, shape=1) +
  geom_abline(
    intercept = mean(sims_minus_18[, 1]),
    slope = mean(sims_minus_18[, 2]),
    size = 1,
    color = "grey50",
    linetype=2
  ) +
  geom_path(data=predpost.loo,aes(x=x,y=y), color="grey50", linetype=2)

p.predloo

```

En la gráfica anterior estamos observando la distribución predictiva posterior de la observación 18.

Calculamos los residuales para ambos modelos. La función `loo_predict` calcula 
de manera agilizada las predicciones para validación utilizando LOO. 

```{r}

fake$residual <- fake$y-fit_all$fitted
fake$looresidual <- fake$y-loo_predict(fit_all)$value

```

```{r}

p1 <- ggplot(fake, aes(x = x, y = residual)) +
  geom_point(color = "black", size = 2, shape=16) +
  geom_point(aes(y=looresidual), color = "grey50", size = 2, shape=1) +
  geom_segment(aes(xend=x, y=residual, yend=looresidual)) +
  geom_hline(yintercept=0, linetype=2) + sin_lineas

p1

```

Los puntos sólidos de la gráfica anterior son los residuales bajo el modelo que utiliza todos los datos, mientras que los puntos sin relleno son los residuales del modelo que deja fuera cada una de las observaciones, es decir, que así como se hizo de dejar fuera la observación 18, este mismo proceso se realiza para el resto de las observaciones.

Si utilizamos todos los datos del modelo, observamos que en promedio los residuales son menores.

Calculamos la desviación estándar de los residuales

```{r }

c(posterior = round(sd(fake$residual),2), 
  loo       = round(sd(fake$looresidual),2), 
  sigma     = sigma)

```

Podemos observar que la desviación estándar con Loo es ligeramente mayor porque como se había mencionado, este modelo sobreajusta y por eso da residuales más grandes.

Calculamos la log-densidad predictiva para cada simulación de nuestro modelo

$$\log \pi( \,y_i \,| x_i, \theta^s \, ), \, \qquad s = 1, \ldots, 10,000\,.$$

```{r }

ll_1 <- log_lik(fit_all)

```

Calculamos la log-densidad predictiva marginalizada para cada observación

$$\log \pi(y_i \, |\, x_i) = \log \left(\frac1S \sum_{s = 1}^S \pi( \,y_i \,| x_i, \theta^s \, ) \right) \,.$$

```{r }

fake$lpd_post <- matrixStats::colLogSumExps(ll_1) - log(nrow(ll_1))

```

Nota que estamos usando `colLogSumExps` para evitar errores numéricos dado que como son densidades, podrían generarse datos muy pequeños y que al aplicarle el logaritmo el cálculo pudiera ser numéricamente inestable.

Calculamos de manera puntual cada log-densidad predictiva sin usar la observación 
$i$-ésima

$$\log \pi(y_i \, |\, x_i) = \log \left(\frac1S \sum_{s = 1}^S \pi( \,y_i \,| x_i, \theta^s_{-i} \, ) \right) \,.$$

```{r }
loo_1 <- loo(fit_all)
fake$lpd_loo <- loo_1$pointwise[,"elpd_loo"] 
```

```{r }
p1 <- ggplot(fake, aes(x = x, y = lpd_post)) +
  geom_point(color = "black", size = 2, shape=16) +
  geom_point(aes(y=lpd_loo), color = "grey50", size = 2, shape=1) +
  geom_segment(aes(xend=x, y=lpd_post, yend=lpd_loo)) +
  ylab("log predictive density") + sin_lineas

p1
```

Lo que se observa en la gráfica anterior es que se tiene un mejor ajuste cuando se incorporan todos los datos en el modelo que cuando dejamos una observación fuera.

## Criterios de información y desempeño de modelos {-}

Regresemos al ejemplo del IQ de los niños en función de algunas características de la madre.

El primer modelo toma en cuenta si la madre terminó o no la preparatoria y su desempeño en la prueba (centrada).

```{r }
fit_kid_m <- stan_glm(kid_score ~ mom_hs + mom_iq_c, data=kidiq,
                  seed=108727, refresh = 0)
fit_kid_m
```

El segundo modelo sólo toma en cuenta si la madre terminó o no la preparatoria.

```{r }
fit_kid_hs <- stan_glm(kid_score ~ mom_hs, data=kidiq,
                  seed=108727, refresh = 0)
fit_kid_hs
```

Calculamos los criterios de información

```{r}
waic.hs <- waic(fit_kid_hs)
waic.hs
```

```{r}
waic.m  <- waic(fit_kid_m)
waic.m
```

Y lo que nos da como resultado es la estimación de la log-densidad predictiva posterior, el número efectivo de parámetros y el criterio de información WAIC en términos de la devianza. También no da las desviaciones estándar de estos cálculos.

Comparando bajo este criterio de información ambos modelos, podemos ver que en términos de la devianza, el segundo modelo es mejor (por ser más chica).

También podemos hacer la comparación de la siguiente forma:

```{r}

loo_compare(waic.hs, waic.m)

```

Esta función nos ordena los modelos del mejor al peor y nos da una estimación de la diferencia en términos de la log-densidad predictiva posterior. También nos da como resultado el error estándar de esta diferencia calculada.

Usando loo

```{r}

loo.hs <- loo(fit_kid_hs)
loo.hs

```

```{r}

loo.m  <- loo(fit_kid_m)
loo.m

```

La ventaja de hacerlo con loo, como se había indicado, es que nos da un diagóstico que nos informa cuando la estimación de devianza es estable e igualmente nos da como resultado las mismas métricas con su desviación estándar.

```{r}

loo_compare(loo.hs, loo.m)

```

### Considerando un modelo mas complejo {-}

Integrando interacciones entre algunas variables.

```{r}

fit_kid_int  <- stan_glm(kid_score ~ mom_hs + mom_iq_c + mom_hs:mom_iq_c,
                  data=kidiq, refresh=0)
fit_kid_int

```

Usamos loo

```{r}

loo.int <- loo(fit_kid_int)
loo.int

```

Comparamos los tres modelos

```{r}

loo_compare(loo.m, loo.int, loo.hs)

```

Y lo que resulta es que pareciera que el mejor modelo es el que integra la interacción entre variables. Sin embargo, existe poca diferencia con la log-densidad predictiva posterior del modelo que tiene dos predictores.

Viendo el ejemplo donde se integraban variables de ruido:

```{r}
fit_kid_noise
loo.noise <- loo(fit_kid_noise)
loo_compare(loo.m, loo.int, loo.hs, loo.noise)

```


**¿Cómo podríamos explicar que la diferencia de las log-densidades predictivas posterior entre el modelo que integra las variables de ruido y el que integra los dos predictores no sea tan diferente?**

Lo que podemos ver en el resultado anterior es que la desviación estándar de las variables de ruido es casi cero mientras que las variables que sí están relacionadas tienen una contribución muy grande, lo que implica que el modelo en promedio se desempeña como el que sólo ocupa las variables relacionadas.

```{r}

fit_kid_cnoise <- stan_glm(kid_score ~ noise, data=kidiqr,
                  seed=108727, refresh = 0)
loo.cnoise <- loo(fit_kid_cnoise)

```

```{r}

loo_compare(loo.m, loo.int, loo.hs, loo.noise, loo.cnoise)

```

Lo que vemos con este resultado es que un modelo que sólo tiene como predictores las variables de ruido, tiene un desempeño muy malo.

```{r}

fit_hs_reg <- stan_glm(kid_score ~ mom_hs + mom_iq_c, prior=hs(), data=kidiq,
                     seed=SEED, refresh = 0)

fit_noise_reg <- stan_glm(kid_score ~ mom_hs + mom_iq_c + noise, prior=hs(),
                      data=kidiqr, seed=SEED, refresh = 0)

print(fit_hs_reg)
print(fit_noise_reg)

loo.hs_reg <- loo(fit_hs_reg)
loo.noise_reg <- loo(fit_noise_reg)

```

Estamos utilizando la distribución previa `horseshoe (hs)` que regulariza los coeficientes y logra que en promedio sean iguales a cero. Es decir, logra un efecto de selección automática de variables.

Comparando los modelos:

```{r}

loo_compare(loo.m, loo.int, loo.hs, loo.noise, loo.cnoise, loo.hs_reg, loo.noise_reg)

```

En general, entre los primeros 5 modelos la capacidad predictiva es similiar. No tenemos evidencia fuerte para descartar alguno de ellos, dado que las diferencias son pequeñas y el error estándar asociado a esta diferencias indica que podría ser que esa diferencia no exista.

## Validación cruzada {-}

```{r}

loo(fit_kid_int)

kfold_10 <- kfold(fit_kid_int, K=10)

print(kfold_10)

```

Como se puede observar este método no estima el número efectivo de parámetros porque no lo incorpora, pero podemos notar que en términos de la devianza loo y validación cruzada son muy similares.